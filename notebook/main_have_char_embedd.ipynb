{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of main_have_char_embedd.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"YgQllUjD0p87","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615976074173,"user_tz":-420,"elapsed":26931,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}},"outputId":"b75af813-ba40-40aa-f4bb-405ea67b4b60"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iMPM2_La0ul8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615976131010,"user_tz":-420,"elapsed":55339,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}},"outputId":"739c9f77-1b6f-4886-a4d2-80170582af4e"},"source":["!pip install flask-ngrok\r\n","!pip install git+https://github.com/facebookresearch/fastText.git"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting flask-ngrok\n","  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n","Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.2)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n","Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n","Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n","Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n","Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n","Installing collected packages: flask-ngrok\n","Successfully installed flask-ngrok-0.0.25\n","Collecting git+https://github.com/facebookresearch/fastText.git\n","  Cloning https://github.com/facebookresearch/fastText.git to /tmp/pip-req-build-z6u6r4z6\n","  Running command git clone -q https://github.com/facebookresearch/fastText.git /tmp/pip-req-build-z6u6r4z6\n","Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (2.6.2)\n","Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (54.0.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (1.19.5)\n","Building wheels for collected packages: fasttext\n","  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3090517 sha256=f2a5b679d1c41f4d798d8c4f6f91804f4065832f1be5fffee427da2d7c3091f2\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-_4vnax9j/wheels/69/f8/19/7f0ab407c078795bc9f86e1f6381349254f86fd7d229902355\n","Successfully built fasttext\n","Installing collected packages: fasttext\n","Successfully installed fasttext-0.9.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d48fmlv7ek7h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615976222804,"user_tz":-420,"elapsed":87563,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}},"outputId":"7e265d2d-a164-403b-c543-59dffc376e2d"},"source":["!pip install tensorflow==1.15\n","!pip install keras==2.2.5\n","!pip install git+https://www.github.com/keras-team/keras-contrib.git"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==1.15\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/2b/e3af15221da9ff323521565fa3324b0d7c7c5b1d7a8ca66984c8d59cb0ce/tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3MB)\n","\u001b[K     |████████████████████████████████| 412.3MB 36kB/s \n","\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.32.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n","Collecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Collecting tensorflow-estimator==1.15.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n","\u001b[K     |████████████████████████████████| 512kB 33.1MB/s \n","\u001b[?25hCollecting keras-applications>=1.0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.2MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.12.4)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.10.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.19.5)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.36.2)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.12.1)\n","Collecting tensorboard<1.16.0,>=1.15.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 42.0MB/s \n","\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.15) (54.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.4)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.7.2)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.1)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=d7c92de01aee8174dfe22c471b3ab6ae28bfeb2dd28a2ac65d2a75bd55344376\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","Installing collected packages: gast, tensorflow-estimator, keras-applications, tensorboard, tensorflow\n","  Found existing installation: gast 0.3.3\n","    Uninstalling gast-0.3.3:\n","      Successfully uninstalled gast-0.3.3\n","  Found existing installation: tensorflow-estimator 2.4.0\n","    Uninstalling tensorflow-estimator-2.4.0:\n","      Successfully uninstalled tensorflow-estimator-2.4.0\n","  Found existing installation: tensorboard 2.4.1\n","    Uninstalling tensorboard-2.4.1:\n","      Successfully uninstalled tensorboard-2.4.1\n","  Found existing installation: tensorflow 2.4.1\n","    Uninstalling tensorflow-2.4.1:\n","      Successfully uninstalled tensorflow-2.4.1\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n","Collecting keras==2.2.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/ba/2d058dcf1b85b9c212cc58264c98a4a7dd92c989b798823cc5690d062bb2/Keras-2.2.5-py2.py3-none-any.whl (336kB)\n","\u001b[K     |████████████████████████████████| 337kB 5.4MB/s \n","\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (2.10.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (3.13)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.0.8)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.15.0)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.19.5)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.4.1)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.1.2)\n","Installing collected packages: keras\n","  Found existing installation: Keras 2.4.3\n","    Uninstalling Keras-2.4.3:\n","      Successfully uninstalled Keras-2.4.3\n","Successfully installed keras-2.2.5\n","Collecting git+https://www.github.com/keras-team/keras-contrib.git\n","  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-n31k5bev\n","  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-n31k5bev\n","Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.2.5)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (2.10.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.19.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.15.0)\n","Building wheels for collected packages: keras-contrib\n","  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp37-none-any.whl size=101065 sha256=29d19675dc4312c7af92b1c7607f3eed4d8470b60cb025d08f4d3b977b012cbc\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-20yf9lev/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n","Successfully built keras-contrib\n","Installing collected packages: keras-contrib\n","Successfully installed keras-contrib-2.0.8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iYwlQRHnfMg1","executionInfo":{"status":"ok","timestamp":1615976230354,"user_tz":-420,"elapsed":1767,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}}},"source":["import numpy as np\n","from numpy import argmax"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"k-XKNXF5AMln","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615976268999,"user_tz":-420,"elapsed":9560,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}},"outputId":"82ea4c47-43e4-4502-95f0-6ac63740c287"},"source":["from keras.models import load_model\n","#https://stackoverflow.com/questions/47266383/save-and-load-weights-in-keras\n","# model_2_3 = load_model(\"/content/drive/MyDrive/model/model_2_3.h5\",custom_objects={'tf': tf})\n","# m.load_weights('/content/drive/MyDrive/model/model_weights_2_3.h5')\n","from keras.models import load_model\n","m = load_model(\"/content/drive/MyDrive/doanh/doanh.h5\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3239: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PKHABcXhSjC3","executionInfo":{"status":"ok","timestamp":1615976283995,"user_tz":-420,"elapsed":6382,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}},"outputId":"7950a6b5-3a12-486e-fc0a-277fa8ab25f9"},"source":["!pip install vncorenlp"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Collecting vncorenlp\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/c2/96a60cf75421ecc740829fa920c617b3dd7fa6791e17554e7c6f3e7d7fca/vncorenlp-1.0.3.tar.gz (2.6MB)\n","\u001b[K     |████████████████████████████████| 2.7MB 4.1MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vncorenlp) (2.23.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2020.12.5)\n","Building wheels for collected packages: vncorenlp\n","  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-cp37-none-any.whl size=2645936 sha256=f95527976bd1194838ce45fae34a9ca903fb47879e89b6f5e7b2610cec22a85b\n","  Stored in directory: /root/.cache/pip/wheels/09/54/8b/043667de6091d06a381d7745f44174504a9a4a56ecc9380c54\n","Successfully built vncorenlp\n","Installing collected packages: vncorenlp\n","Successfully installed vncorenlp-1.0.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EeDy9kM_Sj53","executionInfo":{"status":"ok","timestamp":1615976312279,"user_tz":-420,"elapsed":11923,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}},"outputId":"f59ba631-bfef-4601-c76c-43ee8afbf2ce"},"source":["# get word embedding\n","from vncorenlp import VnCoreNLP\n","print('Creating token word...')\n","annotator = VnCoreNLP(\"/content/drive/MyDrive/Colab Notebooks/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n","print('token word created')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Creating token word...\n","token word created\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vc1MfOWjS_t2"},"source":["# sen = \"ngách 29 ngõ 23 đức diễn xã hương gián huyện yên dũng bắc giang\"\n","# #token\n","# sen_words = annotator.tokenize(sen)[0]\n","# # word embedd\n","# sen_words_embedd = get_word_embbeding(sen_words,max_length=25,embedd_dim = 300)\n","# # char embedd\n","# char_embedd = get_char_embedding(sen_words)\n","# char_embedd = char_embedd.reshape(1,char_embedd.shape[0], char_embedd.shape[1])\n","# print(char_embedd.shape)\n","# # predict\n","# output = m.predict([char_embedd,sen_words_embedd])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D674NiI6Syo9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615976655395,"user_tz":-420,"elapsed":1217,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}},"outputId":"17ddc68e-9be1-475b-9165-6f23a05d5973"},"source":["# aphabet tag library\n","dict_tag = {'pad': 0, '0': 1, 'phan': 2, 'bội_châu': 3, 'Xã': 4, 'Hương_Gián': 5, 'Huyện': 6, 'Yên_Dũng': 7, 'Bắc_Giang': 8, '10000': 9, 'vn': 10, 'sân_bay': 11, 'đang': 12, 'mở_cửa': 13, 'xung_quanh': 14, '1': 15, 'lê_quý': 16, 'đôn': 17, 'Thị_trấn': 18, 'Cái_Dầu': 19, 'Châu_Phú': 20, 'An_Giang': 21, '11000': 22, 'tìm': 23, 'phi_trường': 24, 'quanh': 25, 'đường': 26, 'tỉnh': 27, '427': 28, 'An_Châu': 29, 'Châu_Thành': 30, '12000': 31, 'việt_nam': 32, 'cho': 33, 'tôi': 34, 'quán': 35, 'ba': 36, 'gần': 37, '2': 38, 'lê_trọng': 39, 'tấn': 40, 'Phường': 41, 'Châu_Phú_A': 42, 'Thị_xã': 43, 'Châu_Đốc': 44, 'các': 45, 'bar': 46, 'tại': 47, 'dt': 48, '419': 49, 'Chợ_Mới': 50, '13000': 51, 'vũ_trường': 52, 'cạnh': 53, 'An_Phú': 54, '14000': 55, 'tìm_kiếm': 56, 'rượu': 57, 'bên': 58, '3': 59, 'Mỹ': 60, 'Luông': 61, 'nhà_hàng': 62, '5': 63, 'sao': 64, 'đối_diện': 65, '428': 66, 'Long_Bình': 67, 'thịt': 68, 'nướng': 69, 'ở': 70, 'Chợ': 71, 'Vàm': 72, 'Phú_Tân': 73, '15000': 74, 'bbq': 75, 'Tri_Tôn': 76, '16000': 77, 'liệt_kê': 78, 'bia': 79, '4': 80, 'ngõ': 81, '59': 82, 'Ba': 83, 'Chúc': 84, 'bia_hơi': 85, 'ngách': 86, '638/92': 87, 'Bình_Đức': 88, 'Thành_phố': 89, 'Long_Xuyên': 90, 'cà_phê': 91, 'Chi_Lăng': 92, 'Tịnh_Biên': 93, '17000': 94, 'có': 95, 'vòi_nước': 96, 'uống': 97, 'dùng': 98, 'được': 99, 'nào': 100, 'không': 101, '?': 102, 'không_biết': 103, '18000': 104, 'ăn': 105, 'nhanh': 106, 'không?': 107, 'cửa_hàng': 108, 'đồ_ăn': 109, '466/85': 110, '249': 111, 'Bình_Khánh': 112, 'khu': 113, 'ăn_uống': 114, 'Núi': 115, 'Sam': 116, 'kem': 117, 'tiệm': 118, '19000': 119, '20000': 120, 'nơi': 121, 'bán': 122, '6': 123, 'trường_học': 124, 'đạt': 125, 'chuẩn': 126, 'quốc_gia': 127, '201': 128, 'trường': 129, 'đại_học': 130, 'Phưòng_Đông_Xuyên': 131, 'dạy': 132, 'lái_xe': 133, 'giá': 134, 'rẻ': 135, 'mẫu_giáo': 136, 'quốc_lộ': 137, 'tiểu_học': 138, 'đường': 139, 'tỉnh': 140, '35': 141, 'Mỹ_Long': 142, '7': 143, 'nguyễn_tất_thành': 144, 'Bình': 145, '21000': 146, 'ngôn_ngữ': 147, 'ngoại_ngữ': 148, 'thư_viện': 149, 'Phước': 150, 'liên': 151, 'thôn': 152, 'B': 153, 'công_cộng': 154, '8': 155, 'Quý': 156, 'vui_chơi': 157, '9': 158, 'đường_cao_tốc': 159, 'hà_nội': 160, '-': 161, 'lào_cai': 162, 'Mỹ_Thạnh': 163, '22000': 164, 'âm_nhạc': 165, 'nổi_tiếng': 166, '10': 167, '2a': 168, 'Thới': 169, '23000': 170, 'trung_học': 171, '466/87': 172, 'Vĩnh_Mỹ': 173, '24000': 174, 'trung_học_cơ_sở': 175, '11': 176, 'phố': 177, 'nguyên': 178, 'xá': 179, 'Mỹ_Xuyên': 180, 'cấp': 181, 'hai': 182, 'phú_minh': 183, 'Bình_Chánh': 184, '25000': 185, 'Bình_Long': 186, '26000': 187, 'bãi': 188, 'đỗ': 189, 'xe': 190, '12': 191, 'chùa': 192, 'hưng': 193, 'ký': 194, 'Bình_Mỹ': 195, 'Sơn_Động': 196, 'đậu': 197, 'số': 198, 'Bình_Phú': 199, 'chỗ': 200, 'xe_đạp': 201, '27000': 202, 'xe_máy': 203, '28000': 204, 'ô_tô': 205, '13': 206, '228': 207, 'Bình_Thuỷ': 208, 'hoà_bình': 209, 'An_Hoà': 210, 'sửa': 211, 'Bình_Thạnh': 212, 'Đông': 213, 'thuê': 214, 'Tân_Châu': 215, '29000': 216, '30000': 217, 'thuyền': 218, '14': 219, 'yên': 220, 'sở': 221, 'rửa': 222, '8-3': 223, 'Bình_Hoà': 224, 'trạm': 225, 'bus': 226, 'mái': 227, 'che': 228, 'điểm': 229, '31000': 230, 'dừng': 231, '32000': 232, 'đợi': 233, '15': 234, 'sạc': 235, 'xe_điện': 236, 'free': 237, 'bế': 238, 'văn_đàn': 239, 'nguồn': 240, 'điện': 241, 'bến_xe': 242, 'bến_tàu': 243, 'xăng': 244, '23': 245, '19': 246, 'cây_xăng': 247, 'Cần_Đăng': 248, 'đổ': 249, 'lối': 250, 'vào': 251, '16': 252, 'lý': 253, 'thường': 254, 'kiệt': 255, 'An_Thạnh_Trung': 256, 'Thoại_Sơn': 257, '33000': 258, 'ra': 259, 'Bình_Phước': 260, 'Xuân': 261, 'atm': 262, 'tự_động': 263, 'le': 264, 'hong': 265, 'phong': 266, 'Phú_Mỹ': 267, 'cây': 268, '17': 269, 'lụa': 270, 'Bình_Thành': 271, 'máy': 272, 'rút_tiền': 273, '18': 274, '147': 275, 'Châu_Phong': 276, '34000': 277, 'ngân_hàng': 278, 'thuộc': 279, 'tư_nhân': 280, 'quang_trung': 281, 'Sập': 282, 'Buôn_Ma_Thuột': 283, 'Đắk_Lắk': 284, '35000': 285, 'nhà_băng': 286, 'nguyễn_thị': 287, 'minh': 288, 'khai': 289, 'óc': 290, 'Eo': 291, '36000': 292, 'cơ_sở': 293, 'y_tế': 294, '20': 295, '638/162': 296, 'Phú_Hoà': 297, 'bệnh_viện': 298, '69': 299, 'AnBình': 300, '37000': 301, 'nha_khoa': 302, 'Thắng_Lợi': 303, 'Bắc_Kạn': 304, '38000': 305, 'nhà_thuốc': 306, '21': 307, '287': 308, 'Nhà': 309, 'Bàng': 310, 'hiệu': 311, 'thuốc': 312, 'đức_giang': 313, 'An_Cư': 314, '39000': 315, '40000': 316, 'nhà': 317, 'dưỡng_lão': 318, '22': 319, '7/20': 320, 'Đức': 321, 'thú_y': 322, '422/11/8': 323, 'An_Nông': 324, 'trung_tâm': 325, 'nghệ_thuật': 326, '41000': 327, 'nhà_hát': 328, 'lớn': 329, '42000': 330, 'hội_trường': 331, 'mới': 332, 'xây_dựng': 333, '466/41': 334, 'nhà_chứa': 335, 'hợp_pháp': 336, '97/27': 337, 'sòng': 338, 'bạc': 339, 'An_Tức': 340, 'rạp': 341, 'chiếu_phim': 342, 'đài': 343, 'phun': 344, 'nước': 345, '43000': 346, 'khách_sạn': 347, 'theo': 348, 'giờ': 349, '44000': 350, 'tình_yêu': 351, '24': 352, 'hộp_đêm': 353, '638/37': 354, 'tủ_sách': 355, 'miễn_phí': 356, 'Châu_Lăng': 357, 'đài_truyền_hình': 358, 'phòng': 359, 'thu': 360, 'âm': 361, 'sang_trọng': 362, '466/91': 363, 'studio': 364, 'chụp': 365, 'ảnh': 366, 'cưới': 367, 'học_viện': 368, 'tuyển_sinh': 369, '49': 370, 'Cô_Tô': 371, 'chăn_nuôi': 372, 'rộng': 373, '25': 374, '623': 375, 'Đa_Phước': 376, '45000': 377, 'nông_trại': 378, 'nuôi': 379, 'bò': 380, 'nhân_giống': 381, 'động_vật': 382, 'Đào_Hữu_Cảnh': 383}\n","aphabet_tag = []\n","for key, value in dict_tag.items():\n","  aphabet_tag.append(key)\n","print(aphabet_tag)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["['pad', '0', 'phan', 'bội_châu', 'Xã', 'Hương_Gián', 'Huyện', 'Yên_Dũng', 'Bắc_Giang', '10000', 'vn', 'sân_bay', 'đang', 'mở_cửa', 'xung_quanh', '1', 'lê_quý', 'đôn', 'Thị_trấn', 'Cái_Dầu', 'Châu_Phú', 'An_Giang', '11000', 'tìm', 'phi_trường', 'quanh', 'đường', 'tỉnh', '427', 'An_Châu', 'Châu_Thành', '12000', 'việt_nam', 'cho', 'tôi', 'quán', 'ba', 'gần', '2', 'lê_trọng', 'tấn', 'Phường', 'Châu_Phú_A', 'Thị_xã', 'Châu_Đốc', 'các', 'bar', 'tại', 'dt', '419', 'Chợ_Mới', '13000', 'vũ_trường', 'cạnh', 'An_Phú', '14000', 'tìm_kiếm', 'rượu', 'bên', '3', 'Mỹ', 'Luông', 'nhà_hàng', '5', 'sao', 'đối_diện', '428', 'Long_Bình', 'thịt', 'nướng', 'ở', 'Chợ', 'Vàm', 'Phú_Tân', '15000', 'bbq', 'Tri_Tôn', '16000', 'liệt_kê', 'bia', '4', 'ngõ', '59', 'Ba', 'Chúc', 'bia_hơi', 'ngách', '638/92', 'Bình_Đức', 'Thành_phố', 'Long_Xuyên', 'cà_phê', 'Chi_Lăng', 'Tịnh_Biên', '17000', 'có', 'vòi_nước', 'uống', 'dùng', 'được', 'nào', 'không', '?', 'không_biết', '18000', 'ăn', 'nhanh', 'không?', 'cửa_hàng', 'đồ_ăn', '466/85', '249', 'Bình_Khánh', 'khu', 'ăn_uống', 'Núi', 'Sam', 'kem', 'tiệm', '19000', '20000', 'nơi', 'bán', '6', 'trường_học', 'đạt', 'chuẩn', 'quốc_gia', '201', 'trường', 'đại_học', 'Phưòng_Đông_Xuyên', 'dạy', 'lái_xe', 'giá', 'rẻ', 'mẫu_giáo', 'quốc_lộ', 'tiểu_học', 'đường', 'tỉnh', '35', 'Mỹ_Long', '7', 'nguyễn_tất_thành', 'Bình', '21000', 'ngôn_ngữ', 'ngoại_ngữ', 'thư_viện', 'Phước', 'liên', 'thôn', 'B', 'công_cộng', '8', 'Quý', 'vui_chơi', '9', 'đường_cao_tốc', 'hà_nội', '-', 'lào_cai', 'Mỹ_Thạnh', '22000', 'âm_nhạc', 'nổi_tiếng', '10', '2a', 'Thới', '23000', 'trung_học', '466/87', 'Vĩnh_Mỹ', '24000', 'trung_học_cơ_sở', '11', 'phố', 'nguyên', 'xá', 'Mỹ_Xuyên', 'cấp', 'hai', 'phú_minh', 'Bình_Chánh', '25000', 'Bình_Long', '26000', 'bãi', 'đỗ', 'xe', '12', 'chùa', 'hưng', 'ký', 'Bình_Mỹ', 'Sơn_Động', 'đậu', 'số', 'Bình_Phú', 'chỗ', 'xe_đạp', '27000', 'xe_máy', '28000', 'ô_tô', '13', '228', 'Bình_Thuỷ', 'hoà_bình', 'An_Hoà', 'sửa', 'Bình_Thạnh', 'Đông', 'thuê', 'Tân_Châu', '29000', '30000', 'thuyền', '14', 'yên', 'sở', 'rửa', '8-3', 'Bình_Hoà', 'trạm', 'bus', 'mái', 'che', 'điểm', '31000', 'dừng', '32000', 'đợi', '15', 'sạc', 'xe_điện', 'free', 'bế', 'văn_đàn', 'nguồn', 'điện', 'bến_xe', 'bến_tàu', 'xăng', '23', '19', 'cây_xăng', 'Cần_Đăng', 'đổ', 'lối', 'vào', '16', 'lý', 'thường', 'kiệt', 'An_Thạnh_Trung', 'Thoại_Sơn', '33000', 'ra', 'Bình_Phước', 'Xuân', 'atm', 'tự_động', 'le', 'hong', 'phong', 'Phú_Mỹ', 'cây', '17', 'lụa', 'Bình_Thành', 'máy', 'rút_tiền', '18', '147', 'Châu_Phong', '34000', 'ngân_hàng', 'thuộc', 'tư_nhân', 'quang_trung', 'Sập', 'Buôn_Ma_Thuột', 'Đắk_Lắk', '35000', 'nhà_băng', 'nguyễn_thị', 'minh', 'khai', 'óc', 'Eo', '36000', 'cơ_sở', 'y_tế', '20', '638/162', 'Phú_Hoà', 'bệnh_viện', '69', 'AnBình', '37000', 'nha_khoa', 'Thắng_Lợi', 'Bắc_Kạn', '38000', 'nhà_thuốc', '21', '287', 'Nhà', 'Bàng', 'hiệu', 'thuốc', 'đức_giang', 'An_Cư', '39000', '40000', 'nhà', 'dưỡng_lão', '22', '7/20', 'Đức', 'thú_y', '422/11/8', 'An_Nông', 'trung_tâm', 'nghệ_thuật', '41000', 'nhà_hát', 'lớn', '42000', 'hội_trường', 'mới', 'xây_dựng', '466/41', 'nhà_chứa', 'hợp_pháp', '97/27', 'sòng', 'bạc', 'An_Tức', 'rạp', 'chiếu_phim', 'đài', 'phun', 'nước', '43000', 'khách_sạn', 'theo', 'giờ', '44000', 'tình_yêu', '24', 'hộp_đêm', '638/37', 'tủ_sách', 'miễn_phí', 'Châu_Lăng', 'đài_truyền_hình', 'phòng', 'thu', 'âm', 'sang_trọng', '466/91', 'studio', 'chụp', 'ảnh', 'cưới', 'học_viện', 'tuyển_sinh', '49', 'Cô_Tô', 'chăn_nuôi', 'rộng', '25', '623', 'Đa_Phước', '45000', 'nông_trại', 'nuôi', 'bò', 'nhân_giống', 'động_vật', 'Đào_Hữu_Cảnh']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AB5ojXXev9Dk"},"source":["def get_word_embbeding(sen_words,max_length=25,embedd_dim = 300):\n","  X = np.empty([1, max_length, embedd_dim])\n","  unknown_embedd = np.random.uniform(-0.01, 0.01, [1, 300])\n","  length = len(sen_words)\n","  for i in range(length):\n","    try:\n","      vec = word2vec.get_word_vector(sen_words[i]).tolist()\n","    except:\n","      vec = unknown_embedd\n","    X[0, i, :] = vec\n","  X[0,length:] = np.zeros([1, embedd_dim])\n","  return np.array(X)\n","# sen_words_embedd = get_word_embbeding(sen_words[0],max_length=25,embedd_dim = 300)\n","# print(sen_words_embedd.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oAVbVKV3ZpgG","executionInfo":{"status":"ok","timestamp":1615976686312,"user_tz":-420,"elapsed":1045,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}}},"source":["#get char embedding\n","def read_char_vocab(file_path):\n","  char_vocab = []\n","  for line in open(file_path, encoding='utf-8'):\n","    char_vocab.append(line.splitlines()[0])      \n","  return char_vocab\n","\n","def char_to_int(char_vocab_data):\n","  dic = {}\n","  index = -1\n","  for char in char_vocab_data:\n","    try:\n","      dic[char]\n","    except:\n","      index = index + 1\n","      dic[char]=index\n","  return dic\n"," \n","def get_char_encode(sentence,max_length_of_a_sentence,max_length_of_a_word):\n","  char_vocab_data = read_char_vocab(\"/content/drive/MyDrive/fast_text/VISCII_short.txt\")\n","  dic = char_to_int(char_vocab_data)\n","  # sentence  words is a sentence | ['i','am','an']\n","  sentence_encoded = np.zeros([max_length_of_a_sentence,max_length_of_a_word]) # 25*25\n","  for j in range(len(sentence)):  \n","    word = sentence[j].lower()\n","    # integer_encoded = [char_to_int[char] for char in word]\n","    word_encoded = np.zeros(max_length_of_a_word)\n","    for k in range(len(word)):\n","      char = word[k]\n","      try:\n","        word_encoded[k]= dic[char]\n","      except:\n","        print(\"error : \" + str(char)+\" \"+str(word)+\" \"+str(len(word)))\n","        word_encoded[k]= dic['[unk]']\n","    # sentence encoded\n","    sentence_encoded[j] = word_encoded\n","  return sentence_encoded\n","\n","def get_charEmbedd_form_encode(charEnocde):\n","  LEN_OF_VOCAB = 137\n","  shape = charEnocde.shape\n","  char_embedd = np.zeros([shape[0],shape[1],LEN_OF_VOCAB])\n","  for i in range(shape[0]):\n","    for j in range(shape[1]):\n","      char_int = charEnocde[i,j]\n","      char_int = char_int.astype(np.int64)\n","      onehot = np.zeros(LEN_OF_VOCAB)\n","      onehot[char_int] = 1\n","      char_embedd[i,j,:] = onehot\n","  return char_embedd\n","\n","# return onehot_encoded_of_a_sentence"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mICQPG5QQ-tx","executionInfo":{"status":"ok","timestamp":1614846169055,"user_tz":-420,"elapsed":710,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}},"outputId":"813e0a97-151e-4ca5-b5ce-5d6ec2889d94"},"source":["sen = \"tìm quán thịt chó quanh kiều mai\"\n","sentence = annotator.tokenize(sen)[0]\n","encode = get_char_encode(sentence,25,25)\n","print(encode.shape)\n","char_embedd = get_charEmbedd_form_encode(encode)\n","print(char_embedd.shape)\n","# print(char_embedd[0,0])\n","# print(char_embedd[0,1])\n","# print(char_embedd[0,2])\n","# print(char_embedd[0,3])\n","# print(char_embedd[0,4])\n","# print(char_embedd[0,5])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(25, 25)\n","(25, 25, 137)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zVcSo8z9TJwK"},"source":["# # return [1,2,3,4,5,5,5,5,5,5,5,5,5,5,5]\n","# xxx = []\n","# for i in range(25): \n","#   xxx.append(argmax(output[0,i,:]))\n","\n","# # return [street, homenumber,.....]\n","# yyy=[]\n","# for i in range(11):\n","#   yyy.append(aphabet_tag[xxx[i]])\n","# yyy=np.array(yyy)\n","\n","# # return {'nhaf_hàng': OBJ,....} \n","# zzz={}\n","# for i in range(11):\n","#   zzz[sen_words[i]] = str(yyy[i])\n","# print(zzz)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QkdZwHxH362H","executionInfo":{"status":"ok","timestamp":1615976701574,"user_tz":-420,"elapsed":3506,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}},"outputId":"73529415-06ba-4684-f32f-df1fd22e5e5b"},"source":["!unzip /content/drive/MyDrive/ngrok/ngrok-stable-linux-amd64.zip"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Archive:  /content/drive/MyDrive/ngrok/ngrok-stable-linux-amd64.zip\n","  inflating: ngrok                   \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BloS0Pnz4UA5","executionInfo":{"status":"ok","timestamp":1615976703423,"user_tz":-420,"elapsed":872,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}},"outputId":"5f345daf-968b-4501-a826-56fae00934f6"},"source":["!./ngrok authtoken 1ow8g2aFZ4wILzg2dqHin5bUy5t_34f12x5zn89tGiLLCi4Wy"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L-opCmTfYVEG","executionInfo":{"status":"ok","timestamp":1615977076572,"user_tz":-420,"elapsed":6522,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}},"outputId":"21b78d81-8035-441c-ee2f-a7ec17284257"},"source":["query = \"tìm nhà hàng quanh đây\"\n","try:\n","  sen_words = annotator.tokenize(query)[0]\n","  num_of_word = len(sen_words)\n","except:\n","  print('Creating token word...')\n","  annotator = VnCoreNLP(\"/content/drive/MyDrive/Colab Notebooks/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n","  print('token word created')\n","  sen_words = annotator.tokenize(query)[0]\n","  num_of_word = len(sen_words)\n","print(sen_words)\n","    \n","# get char embedding\n","char_encode = get_char_encode(sen_words,25,25)\n","\n","char_embedd = get_charEmbedd_form_encode(char_encode)\n","char_embedd = char_embedd.reshape(1,char_embedd.shape[0], char_embedd.shape[1],char_embedd.shape[2])\n","\n","# predict\n","output = m.predict([char_embedd])\n","\n","# return [1,2,3,4,5,5,5,5,5,5,5,5,5,5,5] or convert to encode tag\n","tag_encode = []\n","for i in range(25): \n","  tag_encode.append(argmax(output[0,i,:]))\n","\n","# return [street, homenumber,.....] or only get [num_of_word] first element=> convert to tag\n","tag_result=[]\n","for i in range(num_of_word):\n","  tag_result.append(aphabet_tag[tag_encode[i]])\n","  arr_tag_result=np.array(tag_result)\n","\n","print(tag_result)\n","# return {'nhaf_hàng': OBJ,....} \n","result={}\n","for i in range(num_of_word):\n","  result[sen_words[i]] = str(arr_tag_result[i])\n","\n","print(result)\n","#==========================\n","dic_result = {}\n","for word in result:\n","  tag = result[word]\n","  try:\n","    dic_result[tag] += \" \" + word\n","  except:\n","    dic_result[tag] = word\n","print(dic_result)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Creating token word...\n","token word created\n","['tìm', 'nhà_hàng', 'quanh', 'đây']\n","['pad', 'pad', 'pad', 'pad']\n","{'tìm': 'pad', 'nhà_hàng': 'pad', 'quanh': 'pad', 'đây': 'pad'}\n","{'pad': 'tìm nhà_hàng quanh đây'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kssvB0jYrAef","executionInfo":{"status":"ok","timestamp":1615977222948,"user_tz":-420,"elapsed":97683,"user":{"displayName":"An Nguyễn Thành","photoUrl":"","userId":"13004206809231414549"}},"outputId":"1e1a7ae0-da03-4065-9d24-b11c92bb10ca"},"source":["import requests\n","#from flask import Flask, render_template, jsonify\n","from flask import Flask, render_template, abort, request, jsonify, json,Response\n","from flask import request, redirect, url_for\n","import codecs\n","import gensim\n","from distutils.version import LooseVersion, StrictVersion\n","import json\n","from flask_ngrok import run_with_ngrok\n","\n","app = Flask(__name__)\n","run_with_ngrok(app)\n","app.config['JSON_AS_ASCII'] = False\n","\n","global word2vec_model\n","\n","@app.route('/')\n","def summary():\n","  if request.method == \"GET\":\n","    query = request.values['query'] or ''\n","    query = str(query)\n","    print( 'query = ' + query )\n","    \n","    # token word\n","    try:\n","      sen_words = annotator.tokenize(query)[0]\n","      num_of_word = len(sen_words)\n","    except:\n","      print('Creating token word...')\n","      annotator = VnCoreNLP(\"/content/drive/MyDrive/Colab Notebooks/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n","      print('token word created')\n","      sen_words = annotator.tokenize(query)[0]\n","      num_of_word = len(sen_words)\n","    print(sen_words)\n","        \n","    # get char embedding\n","    char_encode = get_char_encode(sen_words,25,25)\n","\n","    char_embedd = get_charEmbedd_form_encode(char_encode)\n","    char_embedd = char_embedd.reshape(1,char_embedd.shape[0], char_embedd.shape[1],char_embedd.shape[2])\n","\n","    # predict\n","    output = m.predict([char_embedd])\n","\n","    # return [1,2,3,4,5,5,5,5,5,5,5,5,5,5,5] or convert to encode tag\n","    tag_encode = []\n","    for i in range(25): \n","      tag_encode.append(argmax(output[0,i,:]))\n","\n","    # return [street, homenumber,.....] or only get [num_of_word] first element=> convert to tag\n","    tag_result=[]\n","    for i in range(num_of_word):\n","      tag_result.append(aphabet_tag[tag_encode[i]])\n","      arr_tag_result=np.array(tag_result)\n","\n","    print(tag_result)\n","\n","    return jsonify(sen_words=sen_words, tags=tag_result)\n","if __name__ == \"__main__\":\n","  import os\n","  app.run()"],"execution_count":16,"outputs":[{"output_type":"stream","text":[" * Serving Flask app \"__main__\" (lazy loading)\n"," * Environment: production\n","\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n","\u001b[2m   Use a production WSGI server instead.\u001b[0m\n"," * Debug mode: off\n"],"name":"stdout"},{"output_type":"stream","text":[" * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"],"name":"stderr"},{"output_type":"stream","text":[" * Running on http://7bbba1c11f56.ngrok.io\n"," * Traffic stats available on http://127.0.0.1:4040\n"],"name":"stdout"},{"output_type":"stream","text":["127.0.0.1 - - [17/Mar/2021 10:32:10] \"\u001b[31m\u001b[1mGET / HTTP/1.1\u001b[0m\" 400 -\n","127.0.0.1 - - [17/Mar/2021 10:32:11] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"],"name":"stderr"},{"output_type":"stream","text":["query = tam phi_arường đang mở_cửa qaanh đaờng tỉna 4a7\n","Creating token word...\n"],"name":"stdout"},{"output_type":"stream","text":["127.0.0.1 - - [17/Mar/2021 10:32:27] \"\u001b[37mGET /?query=tam%20phi_arường%20đang%20mở_cửa%20qaanh%20đaờng%20tỉna%204a7 HTTP/1.1\u001b[0m\" 200 -\n"],"name":"stderr"},{"output_type":"stream","text":["token word created\n","['tam', 'phi', '_', 'arường', 'đang', 'mở', '_', 'cửa', 'qaanh', 'đaờng', 'tỉna', '4a7']\n","['pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n","query = tam phi arường đang mở cửa qaanh đaờng tỉna 4a7\n","Creating token word...\n"],"name":"stdout"},{"output_type":"stream","text":["127.0.0.1 - - [17/Mar/2021 10:33:26] \"\u001b[37mGET /?query=tam%20phi%20arường%20đang%20mở%20cửa%20qaanh%20đaờng%20tỉna%204a7 HTTP/1.1\u001b[0m\" 200 -\n"],"name":"stderr"},{"output_type":"stream","text":["token word created\n","['tam', 'phi', 'arường', 'đang', 'mở_cửa', 'qaanh', 'đaờng', 'tỉna', '4a7']\n","['pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n"],"name":"stdout"}]}]}